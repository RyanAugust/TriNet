{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import build_dataset\n",
    "\n",
    "lfx = build_dataset.load_functions()\n",
    "pfx = build_dataset.performance_functions()\n",
    "\n",
    "pp = build_dataset.dataset_preprocess(local_activity_store='gc_activitydata_local.csv'\n",
    "                                    ,local_activity_model_params='modeled_ef.csv')\n",
    "lfx.metric_function_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfx.metric_function_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_df = lfx.derive_load(frame=pp.activity_data, load_metric='TIZ1_3')\n",
    "loaded_df = lfx.derive_load(frame=pp.activity_data, load_metric='TIZ2_3')\n",
    "loaded_df = lfx.derive_load(frame=pp.activity_data, load_metric='TIZ3_3')\n",
    "loaded_df = loaded_df[loaded_df['TIZ1_3'].cumsum() > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfx.derive_performance(loaded_df, performance_metric='VO2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_original[data_original['Sport'] == 'Bike'].copy()\n",
    "data['mod_pow_at_threshold'] = data['a'] + data['b']*athlete_statics['threshold_hr'] +  data['c']*(60*60)*20\n",
    "\n",
    "data = data.groupby('date').agg({'L1_Time_in_Zone':'sum'\n",
    "                                ,'L2_Time_in_Zone':'sum'\n",
    "                                ,'L3_Time_in_Zone':'sum'\n",
    "                                ,'L4_Time_in_Zone':'sum'\n",
    "                                ,'L5_Time_in_Zone':'sum'\n",
    "                                ,'L6_Time_in_Zone':'sum'\n",
    "                                ,'L7_Time_in_Zone':'sum'\n",
    "                                ,'mod_pow_at_threshold':'max'}).reset_index()\n",
    "\n",
    "# data['date'] = data.index\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data = data.sort_values(by=['date'])\n",
    "data.index = pd.DatetimeIndex(data['date'])\n",
    "missing_dates = pd.date_range(start=data.index.min(), end=data.index.max())\n",
    "data = data.reindex(missing_dates, fill_value=0)\n",
    "data['mod_pow_at_threshold'] = data['mod_pow_at_threshold'].replace(0,np.nan)\n",
    "data['mod_pow_at_threshold'] = data['mod_pow_at_threshold'].fillna(method='ffill')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TimeSeriesCNN, self).__init__()\n",
    "        \n",
    "        # Define the layers of the CNN\n",
    "        self.conv1 = nn.Conv1d(in_channels=8, out_channels=32, kernel_size=3)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(in_features=64*8, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the layers of the CNN\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64*8)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_df = loaded_df[loaded_df['Sport'].isin(['Bike','Run'])][['date','Sport','TIZ1_3','TIZ2_3','TIZ3_3','VO2']].groupby(['date','Sport']).agg({'TIZ1_3':'sum','TIZ2_3':'sum','TIZ3_3':'sum','VO2':'max'})\n",
    "ul_df = sl_df.unstack(level=1).copy()\n",
    "ul_df.fillna({'TIZ1_3':0\n",
    "             ,'TIZ2_3':0\n",
    "             ,'TIZ3_3':0\n",
    "            }).fillna(method='ffill', limit=5, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_vo2 = ul_df[ul_df['VO2'].isnull().sum(axis=1) < 1]['VO2']\n",
    "bike_to_run = (comp_vo2['Bike'] / comp_vo2['Run']).mean()\n",
    "comp_vo2['Run from bike'] = comp_vo2['Bike'] * 1/bike_to_run\n",
    "ax = comp_vo2.plot(kind='scatter', x = 'Run', y='Run from bike', xlim=(50,75), ylim=(50,75), figsize=(4,4))\n",
    "sing_vo2 = ul_df[ul_df['VO2'].isnull().sum(axis=1) < 2]['VO2']\n",
    "ul_df[('VO2','Run')].fillna(ul_df[('VO2','Bike')] * 1/bike_to_run, inplace=True)\n",
    "ul_df[('VO2','Bike')].fillna(ul_df[('VO2','Run')] * bike_to_run, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_df = ul_df[['TIZ1_3','TIZ2_3','TIZ3_3']]\n",
    "fl_df = fl_df.join(ul_df[['VO2']])\n",
    "fl_df[[('VO2_l1','Bike'),('VO2_l1','Run')]] = fl_df['VO2'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = fl_df[['TIZ1_3','TIZ2_3','TIZ3_3','VO2_l1']].to_numpy()[:-1]\n",
    "target_data = fl_df['VO2'][:-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_percent = 0.8\n",
    "test_split = int(input_data.shape[0] * test_percent)\n",
    "train_input_data, train_target_data = input_data[:test_split], target_data[:test_split]\n",
    "test_input_data, test_target_data = input_data[test_split:], target_data[test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_data[:32].T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeSeriesCNN()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    inputs = torch.from_numpy(train_input_data[:32].T).float()\n",
    "    targets = torch.from_numpy(train_target_data).float()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.from_numpy(test_data).float()\n",
    "            targets = torch.from_numpy(test_labels).float()\n",
    "            outputs = model(inputs)\n",
    "            test_loss = criterion(outputs, targets)\n",
    "            print(f\"Epoch {epoch}: Test loss = {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, min_index=0, max_index=None, batch_size=16, n_steps=150, step_length=1000):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - 1\n",
    "     \n",
    "    while True:\n",
    "        # Pick indices of ending positions\n",
    "        rows = np.random.randint(min_index + n_steps * step_length, max_index, size=batch_size)\n",
    "         \n",
    "        # Initialize feature matrices and targets\n",
    "        samples = np.zeros((batch_size, n_steps, n_features))\n",
    "        targets = np.zeros(batch_size, )\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            samples[j] = create_X(data[:, 0], last_index=row, n_steps=n_steps, step_length=step_length)\n",
    "            targets[j] = data[row - 1, 1]\n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shaping of input data\n",
    "data = np.random.rand(1000, 50, 10)\n",
    "labels = np.random.rand(1000, 1)\n",
    "input_data = np.random.rand(1000, 50, 10)\n",
    "labels = np.random.rand(1000, 1)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, train_labels = data[:800], labels[:800]\n",
    "test_data, test_labels = data[800:], labels[800:]\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 10\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "model = RNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    inputs = torch.from_numpy(train_data).float()\n",
    "    targets = torch.from_numpy(train_labels).float()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.from_numpy(test_data).float()\n",
    "            targets = torch.from_numpy(test_labels).float()\n",
    "            outputs = model(inputs)\n",
    "            test_loss = criterion(outputs, targets)\n",
    "            print(f\"Epoch {epoch}: Test loss = {test_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "786d347783ef61a22e711d613621ccd6d628c0aed896bdd427ccc0f1fe525ed4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
