{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4a2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import math\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "# import arviz as az\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f05525",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d5564",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('Trying local data')\n",
    "    data_original = pd.read_csv('./gc_activitydata_ryan.csv', parse_dates=['date'])\n",
    "    modeled_df =  pd.read_csv('./modeled_ef.csv')\n",
    "    del modeled_df['Unnamed: 0']\n",
    "    data_original = pd.merge(data_original, modeled_df, left_on='filename',right_on='files', how='left')\n",
    "except:\n",
    "    print('Using GC API')\n",
    "    data_original = pd.read_csv(\n",
    "        'http://localhost:12021/Ryan%20Duecker?metrics=Duration,TSS,StrydStress,Average_Heart_Rate,Max_Heartrate,Average_Power,\\\n",
    "Athlete_Weight,Estimated_VO2MAX,10_sec_Peak_Pace_Swim,xPace,Pace,IsoPower,Power_Index\\\n",
    ",L1_Time_in_Zone,L2_Time_in_Zone,L3_Time_in_Zone,L4_Time_in_Zone,L5_Time_in_Zone,L6_Time_in_Zone,L7_Time_in_Zone&\\\n",
    "metadata=VO2max_detected,Shoes,Workout_Code,Workout_Title,Indoor,Frame,Sport'\n",
    "    )\n",
    "    data_original.columns = [x.strip(' \"') for x in data_original.columns]\n",
    "\n",
    "    data_original['Sport'] = np.where(data_original['StrydStress']>0\n",
    "                                 ,'Run'\n",
    "                                 ,np.where(data_original['Average_Power']>0\n",
    "                                     ,'Bike'\n",
    "                                     ,np.where(data_original['10_sec_Peak_Pace_Swim']>0\n",
    "                                         ,'Swim'\n",
    "                                         ,'Other')))\n",
    "    data_original['date'] = pd.to_datetime(data_original['date'])\n",
    "    data_original['VO2max_detected'] = data_original['VO2max_detected'].astype(float)\n",
    "    #Save GC data locally\n",
    "    data_original.to_csv('gc_activitydata_ryan.csv', index=False)\n",
    "\n",
    "    # for this build, add in ef data\n",
    "    modeled_df =  pd.read_csv('./modeled_ef.csv')\n",
    "    del modeled_df['Unnamed: 0']\n",
    "    data_original = pd.merge(data_original, modeled_df, left_on='filename',right_on='files', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd73c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "athlete_statics = {\"max_hr\": 191\n",
    "                  ,\"resting_hr\": 40\n",
    "                  ,'ae_threshold_hr': 148\n",
    "                  ,'threshold_hr': 168\n",
    "                  ,'run_settings':{'cp': 356,\n",
    "                'w_prime': 16900,\n",
    "                'pmax': 642}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ef6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_index_maker(power, duration, cp=340, w_prime=15000, pmax=448):\n",
    "    theoretical_power = w_prime/duration - w_prime/(cp-pmax) + cp\n",
    "    power_index = (power/theoretical_power)*100\n",
    "    return power_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_original['Power_Index'] = np.where(data_original['Sport'] == 'Run'\n",
    "                                        ,power_index_maker(power=data_original['Average_Power'], duration=data_original['Duration'], **athlete_statics['run_settings'])\n",
    "                                        ,data_original['Power_Index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a041d585",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "___\n",
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c570a8-c13f-41b7-9eac-b42d146497e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(data_df, performance_fxn, performance_lower_bound=0, sport=False):\n",
    "    data_df['xPace'] = np.where(data_df['xPace'] <= 0, data_df['Pace'], data_df['xPace'])\n",
    "    data_df = data_df[~(((data_df['Sport'] == 'Run') & (data_df['Pace'] <= 0))\n",
    "               | ((data_df['Sport'] == 'Bike') & (data_df['Average_Power'] <= 0))\n",
    "               | (data_df['Average_Heart_Rate'] <= 0))].copy()\n",
    "    data_df.rename(columns={'date':'workoutDate'}, inplace=True)\n",
    "    data_df['day_TSS'] = data_df['TSS'].groupby(data_df['workoutDate']).transform('sum').fillna(0)\n",
    "    data_df['performance_metric'] = data_df.apply(lambda row: performance_fxn(row, athlete_statics), axis=1)\n",
    "    # data_df['performance_metric'] = np.where(data_df['Duration'] < 60*60, 0, data_df['performance_metric'])\n",
    "    data_df['performance_metric'] = np.where(data_df['performance_metric'] < performance_lower_bound, 0, data_df['performance_metric'])\n",
    "    \n",
    "    # data_df = data_df[['workoutDate','day_TSS','performance_metric','Sport']]\n",
    "\n",
    "    data_df['performance_metric'] = data_df['performance_metric'].replace(0,np.nan)\n",
    "    data_df['performance_metric'] = data_df['performance_metric'].fillna(method='ffill')\n",
    "    agg_dict = {'day_TSS':'mean','performance_metric':'max'}\n",
    "    if sport:\n",
    "        agg_dict.update({'Sport':'first'})\n",
    "        data_df = data_df.sort_values('Sport')\n",
    "    data_df = data_df.groupby('workoutDate').agg(agg_dict)\n",
    "    \n",
    "    data_df['date'] = data_df.index\n",
    "    data_df['date'] = pd.to_datetime(data_df['date'])\n",
    "    data_df = data_df.sort_values(by=['date'])\n",
    "    data_df.index = pd.DatetimeIndex(data_df['date'])\n",
    "    missing_dates = pd.date_range(start=data_df.index.min(), end=data_df.index.max())\n",
    "    data_df = data_df.reindex(missing_dates, fill_value=0)\n",
    "    data_df['performance_metric'] = data_df['performance_metric'].replace(0,np.nan)\n",
    "    data_df['performance_metric'] = data_df['performance_metric'].fillna(method='ffill')\n",
    "    data_df = data_df.dropna()\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1810f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vo2(row, athlete_statics):\n",
    "    if row['Sport'] == 'Bike':\n",
    "        percent_vo2 = (row['Average_Heart_Rate'] - athlete_statics[\"resting_hr\"])/(athlete_statics[\"max_hr\"] - athlete_statics[\"resting_hr\"])\n",
    "        vo2_estimated = (((row['Average_Power']/75)*1000)/row['Athlete_Weight']) / percent_vo2\n",
    "    elif row['Sport'] == 'Run':\n",
    "        percent_vo2 = (row['Average_Heart_Rate'] - athlete_statics[\"resting_hr\"])/(athlete_statics[\"max_hr\"] - athlete_statics[\"resting_hr\"])\n",
    "        vo2_estimated = (210/row['xPace']) / percent_vo2\n",
    "    else:\n",
    "        vo2_estimated =  0\n",
    "    return vo2_estimated\n",
    "\n",
    "def use_garmin_vo2(row, athlete_statics):\n",
    "    vo2_estimated = 0\n",
    "    if (row['Workout_Code'] != 'Rec') & (row['Sport'] in ['Run','Bike']):\n",
    "        vo2_estimated = row['VO2max_detected'] # Garmin VO2 Estimation\n",
    "    return vo2_estimated\n",
    "\n",
    "def calc_ae_ef(row, athlete_statics):\n",
    "    ef = 0\n",
    "    if (row['Workout_Code'] == 'AE'):\n",
    "        if row['Sport'] == 'Bike':\n",
    "            ef = row['IsoPower']/row['Average_Heart_Rate']\n",
    "        elif row['Sport'] == 'Run':\n",
    "            ef = row['IsoPower']/row['Average_Heart_Rate']\n",
    "    return ef\n",
    "\n",
    "def use_power_index(row, athlete_statics):\n",
    "    if row['Average_Power'] > 0:\n",
    "        val = row['Power_Index']\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "def use_power_index_ef(row, athlete_statics):\n",
    "    if row['Average_Power'] > 0:\n",
    "        hr_range = athlete_statics['max_hr'] - athlete_statics['resting_hr']\n",
    "        avg_hr_rel = row['Average_Heart_Rate'] - athlete_statics['resting_hr']\n",
    "        relative_hr = (avg_hr_rel / hr_range)*100\n",
    "        \n",
    "        pi_ef = row['Power_Index']/relative_hr\n",
    "        val = pi_ef\n",
    "    else:\n",
    "        val = 0\n",
    "    return val\n",
    "\n",
    "def modeled_aerobic_threshold_power(row, athlete_statics):\n",
    "    temp = 20\n",
    "    duration = 60*60\n",
    "    \n",
    "    if (row['a'] != 0) & (row['Duration'] > 999):\n",
    "        power = row['a'] + row['b']*athlete_statics['threshold_hr'] +  row['c']*duration*temp\n",
    "        return power\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "performance_fxns = [calc_vo2, use_garmin_vo2, calc_ae_ef, use_power_index, use_power_index_ef, modeled_aerobic_threshold_power]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcf19f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_original.copy()\n",
    "data['mod_ef_at_threshold'] = data['a'] + data['b']*athlete_statics['threshold_hr'] +  data['c']*(60*60)*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc86ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(8,4))\n",
    "(data[['a','b','c']]/data[['a','b','c']].mean()).plot(kind='box', ax=axs[0], title='EF(mean norm) coef vals')\n",
    "data['a'].plot(kind='hist',bins=100, ax=axs[1], title='Intercept Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[(data['mod_ef_at_threshold'] > 350) & (data['Sport'] == 'Bike')][['Sport','Duration','date','Average_Heart_Rate','Average_Power','mod_ef_at_threshold','a','b','c']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28052f57",
   "metadata": {},
   "source": [
    "## Check weekly readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49488bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data_original[data_original['Sport'] == 'Bike'].copy()\n",
    "# data = pre_process(data, performance_fxn=calc_ae_ef, performance_lower_bound=0)\n",
    "\n",
    "# data = data.reset_index()\n",
    "# data['week_start']  = data['index'].apply(lambda day_instance: day_instance - datetime.timedelta(days=day_instance.day_of_week))\n",
    "# data.set_index('index', inplace=True)\n",
    "\n",
    "# summary_data = data.groupby(['week_start']).agg({'day_TSS':'sum','performance_metric':'max'})\n",
    "# # summary_data = summary_data[:-1]\n",
    "# fig, ax = plt.subplots(2,1, figsize=(10,8), dpi=100)\n",
    "# ax[0].bar(summary_data.index, summary_data['day_TSS'])\n",
    "# ax0_1 = ax[0].twinx()\n",
    "# ax0_1.scatter(summary_data.index, summary_data['performance_metric'], s=5, label='performance_metric est.', color='red')\n",
    "# ax0_1.plot(summary_data.index, summary_data['performance_metric'], ls='-', lw=1, color='red')\n",
    "\n",
    "# ax[1].bar(data.index, data['day_TSS'])\n",
    "# ax1_1 = ax[1].twinx()\n",
    "# ax1_1.scatter(data.index, data['performance_metric'], s=5, label='performance_metric est.', color='red')\n",
    "# ax1_1.plot(data.index, data['performance_metric'], ls='-', lw=1, color='red')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d212760-9fbb-42ec-a6b9-d160340fc181",
   "metadata": {},
   "source": [
    "## Scipy Solver\n",
    "___\n",
    "### Settings & Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c07d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class banister(object):\n",
    "    def __init__(self, params=[0.1, 0.5, 50, 45, 7], ctlatl_start=0):\n",
    "        self.params = params\n",
    "        self.ctls = []\n",
    "        self.atls = []\n",
    "        self.ctlatl_start = [ctlatl_start]\n",
    "    \n",
    "    def model(self, load_metric, params=[]):\n",
    "        if len(params) != 5:\n",
    "            params = self.params\n",
    "        self.params = params\n",
    "        \n",
    "        self.ctls = self.atls = self.ctlatl_start # why?\n",
    "        Banister_Predictions = np.array([])\n",
    "        for i in range(len(load_metric)):\n",
    "            ctl = (load_metric[i] * (1-math.exp(-1/params[3]))) + (self.ctls[i] * (math.exp(-1/params[3])))\n",
    "            atl = (load_metric[i] * (1-math.exp(-1/params[4]))) + (self.atls[i] * (math.exp(-1/params[4])))\n",
    "            self.ctls.append(ctl)\n",
    "            self.atls.append(atl)\n",
    "            Banister_Predictions = np.append(Banister_Predictions, params[2] + params[0]*ctl - params[1]*atl)\n",
    "\n",
    "        return Banister_Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed865c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctls_ref = 42\n",
    "atls_ref = 7\n",
    "performance_fxn = modeled_aerobic_threshold_power #performance_fxns[-1]\n",
    "\n",
    "\n",
    "data_pre = data_original[\n",
    "                    (data_original['date'] > pd.to_datetime(datetime.date(year=2022, month=5, day=25)))\n",
    "                    & (data_original['date'] < pd.to_datetime(datetime.date(year=2023, month=7, day=26)))\n",
    "                    & (data_original['Sport'] == 'Bike')\n",
    "                    ].copy()\n",
    "# data_pre = data_original.copy()\n",
    "data = pre_process(data_pre, performance_fxn=performance_fxn)\n",
    "load_metric = data['day_TSS'].to_numpy()\n",
    "performance_metric = data['performance_metric'].to_numpy()\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "data['day_TSS'].rolling(window=7,min_periods=1).sum().plot(ax=ax)\n",
    "ax2 = ax.twinx()\n",
    "ax2.scatter(data.index\n",
    "            ,data['performance_metric']#.rolling(window=1, min_periods=1).max()\n",
    "            ,marker='o'\n",
    "            ,edgecolor='red'\n",
    "            ,facecolor='none'\n",
    "            ,label='performance metric'\n",
    "            )\n",
    "fig.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1d633e7",
   "metadata": {},
   "source": [
    "### Execute Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = banister()\n",
    "\n",
    "def optimize_banister(params):    \n",
    "    losses = []\n",
    "\n",
    "    Banister_Predictions = bm.model(load_metric, params=params)\n",
    "    \n",
    "    losses = abs(performance_metric - Banister_Predictions)\n",
    "    MAE = np.mean(losses)\n",
    "    return MAE\n",
    "\n",
    "initial_guess  = [0, 0, 270, 45, 7]\n",
    "individual_banister_model = optimize.minimize(optimize_banister\n",
    "                                            ,x0=initial_guess\n",
    "                                                #     k1,       k2,       p0,     CTLS,   ATLS\n",
    "                                            ,bounds=[(.001,1.90),(.001,2.90),(200,300),(30,50),(5,12)]\n",
    "                                            # ,method='Nelder-Mead'\n",
    "                                            ,tol=1e-8\n",
    "                                            )\n",
    "print(individual_banister_model)\n",
    "for val in individual_banister_model['x']:\n",
    "    print(val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e522376f",
   "metadata": {},
   "source": [
    "### Assess accuracy of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd098c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_metric = data['day_TSS'].tolist()\n",
    "data['pred performance_metric'] = bm.model(load_metric)\n",
    "\n",
    "p_data = data[data['performance_metric']<data['performance_metric'].quantile(.90)]\n",
    "\n",
    "## Plot\n",
    "fig, ax = plt.subplots(2,1, figsize=(8,12), dpi=100,  gridspec_kw={'height_ratios': [2, 1]})\n",
    "ax[0].scatter(x=p_data['performance_metric']\n",
    "          ,y=p_data['pred performance_metric']\n",
    "          ,s=4\n",
    "          ,alpha=.5)\n",
    "ax[0].set_xlabel('actual preformance')\n",
    "ax[0].set_ylabel('predicted preformance')\n",
    "\n",
    "ax[1].scatter(p_data.index\n",
    "             ,p_data['performance_metric']\n",
    "             ,label='actual'\n",
    "             ,facecolor=None\n",
    "             ,edgecolor='blue'\n",
    "             ,s=5\n",
    "             ,linewidths=1\n",
    "             ,marker='o')\n",
    "ax[1].scatter(p_data.index\n",
    "             ,p_data['pred performance_metric']\n",
    "             ,label='pred'\n",
    "             ,facecolor=None\n",
    "             ,edgecolor='red'\n",
    "             ,s=5\n",
    "             ,linewidths=1\n",
    "             ,marker='o')\n",
    "ax[1].set_xlabel('date')\n",
    "ax[1].set_xlabel('performance')\n",
    "ax[1].legend(loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0e2a5-23dc-4cb4-85d1-d6d4647a9472",
   "metadata": {},
   "source": [
    "## Bayes Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6abf30-7efe-4964-89f2-e2c8e9445eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "from pymc import HalfCauchy, Model, Normal, sample, Uniform, Beta\n",
    "\n",
    "print(f\"Running on PyMC v{pm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f80666f-8892-44ce-a872-c5815dbf198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pre = data_original[data_original['Sport'] == 'Run'].copy()\n",
    "data = pre_process(data_pre, performance_fxn=performance_fxn)\n",
    "    \n",
    "TSS = data['day_TSS'].tolist()\n",
    "performance_kpi = data['performance_metric'].tolist()\n",
    "\n",
    "def banister_regression(TSS, performance_kpi):\n",
    "    with Model() as model:  # model specifications in PyMC are wrapped in a with-statement\n",
    "        # Define priors\n",
    "        sigma = Normal(\"sigma\", 0, sigma=5)\n",
    "\n",
    "        p0 = Normal(\"p0\", 55, sigma=7)\n",
    "        ctls_mu = Normal(\"ctls_mu\", 42, 4)\n",
    "        atls_mu = Normal(\"atls_mu\", 7, 2)\n",
    "        ctls_sigma = Normal(\"ctls_sigma\", 8, 4)\n",
    "        atls_sigma = Normal(\"atls_sigma\", 4, 2)\n",
    "        ctls = Normal(\"ctls\", ctls_mu, sigma=ctls_sigma) #6\n",
    "        atls = Normal(\"atls\", atls_mu, sigma=atls_sigma) #2.5\n",
    "        \n",
    "        k1 = Beta(\"k1\", alpha=1, beta=5) # guess == 0.1\n",
    "        k2 = Beta(\"k2\", alpha=1, beta=3) # guess == 0.5\n",
    "\n",
    "        # Define likelihood\n",
    "        banister_prediction = Normal(\"banister_prediction\",\n",
    "                                     mu=(p0 \n",
    "                                         + k1 * (TSS * 1-pm.math.exp(-1/ctls))\n",
    "                                         - k2 * (TSS * 1-pm.math.exp(-1/atls))\n",
    "                                        )\n",
    "                                    ,sigma=sigma\n",
    "                                    ,observed=performance_kpi)\n",
    "    return model\n",
    "\n",
    "banister_model = banister_regression(TSS=TSS, performance_kpi=performance_kpi)\n",
    "with banister_model:\n",
    "    fit = pm.sample(2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef11e56-16c4-4a70-bbff-9b4a3f551e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "az.plot_posterior(fit, var_names=[\"k1\"], ref_val=.1, ax=ax[0][0])\n",
    "ax[0][0].set(title=\"k1 Posterior\", xlabel=\"k1\")\n",
    "\n",
    "az.plot_posterior(fit, var_names=[\"k2\"], ref_val=.5, ax=ax[0][1])\n",
    "ax[0][1].set(title=\"k2 Posterior\", xlabel=\"k2\")\n",
    "\n",
    "az.plot_posterior(fit, var_names=[\"p0\"], ref_val=55, ax=ax[0][2])\n",
    "ax[0][2].set(title=\"p0 Posterior\", xlabel=\"p0\")\n",
    "\n",
    "az.plot_posterior(fit, var_names=[\"sigma\"], ref_val=0, ax=ax[0][3])\n",
    "ax[0][3].set(title=\"Sigma Posterior\", xlabel=\"Sigma\")\n",
    "\n",
    "\n",
    "az.plot_posterior(fit, var_names=[\"ctls\"], ref_val=ctls_ref, ax=ax[1][0])\n",
    "ax[1][0].set(title=\"CTLS Posterior\", xlabel=\"CTLS\")\n",
    "\n",
    "az.plot_posterior(fit, var_names=[\"atls\"], ref_val=atls_ref, ax=ax[1][1])\n",
    "ax[1][1].set(title=\"ATLS Posterior\", xlabel=\"ATLS\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80617a55",
   "metadata": {},
   "source": [
    "## NN Optimization\n",
    "___\n",
    "TIZ input trained on modeled Power@threshold HR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d6bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_original[data_original['Sport'] == 'Bike'].copy()\n",
    "data['mod_pow_at_threshold'] = data['a'] + data['b']*athlete_statics['threshold_hr'] +  data['c']*(60*60)*20\n",
    "\n",
    "data = data.groupby('date').agg({'L1_Time_in_Zone':'sum'\n",
    "                                ,'L2_Time_in_Zone':'sum'\n",
    "                                ,'L3_Time_in_Zone':'sum'\n",
    "                                ,'L4_Time_in_Zone':'sum'\n",
    "                                ,'L5_Time_in_Zone':'sum'\n",
    "                                ,'L6_Time_in_Zone':'sum'\n",
    "                                ,'L7_Time_in_Zone':'sum'\n",
    "                                ,'mod_pow_at_threshold':'max'}).reset_index()\n",
    "\n",
    "# data['date'] = data.index\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data = data.sort_values(by=['date'])\n",
    "data.index = pd.DatetimeIndex(data['date'])\n",
    "missing_dates = pd.date_range(start=data.index.min(), end=data.index.max())\n",
    "data = data.reindex(missing_dates, fill_value=0)\n",
    "data['mod_pow_at_threshold'] = data['mod_pow_at_threshold'].replace(0,np.nan)\n",
    "data['mod_pow_at_threshold'] = data['mod_pow_at_threshold'].fillna(method='ffill')\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193d06ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_base = data[['L1_Time_in_Zone',\n",
    "'L2_Time_in_Zone',\n",
    "'L3_Time_in_Zone',\n",
    "'L4_Time_in_Zone',\n",
    "'L5_Time_in_Zone',\n",
    "'L6_Time_in_Zone',\n",
    "'L7_Time_in_Zone']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceaf0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_base = data[['L1_Time_in_Zone',\n",
    "# 'L2_Time_in_Zone',\n",
    "# 'L3_Time_in_Zone',\n",
    "# 'L4_Time_in_Zone',\n",
    "# 'L5_Time_in_Zone',\n",
    "# 'L6_Time_in_Zone',\n",
    "# 'L7_Time_in_Zone']].to_numpy()\n",
    "# window_size = 50\n",
    "# for seg in range(X_base.shape[0] - window_size):\n",
    "#     if seg > 0:\n",
    "#         new = X_base[seg:seg+window_size]\n",
    "#         X = np.concatenate((X,[new]))\n",
    "#     else:\n",
    "#         X = np.array([X_base[seg:seg+window_size]])\n",
    "# X = torch.tensor(X)\n",
    "# X.shape\n",
    "# y = torch.tensor(data['mod_pow_at_threshold'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d6a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "#         # self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=7, stride=1)\n",
    "#         self.fc0 = nn.Linear(in_features=7, out_features=3)\n",
    "#         # kernel\n",
    "#         self.fc1 = nn.Linear(7 * 7 * 3, 32)  # 5*5 from image dimension\n",
    "#         self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.conv1(x))\n",
    "#         x = F.max_pool2d(x, 2)\n",
    "\n",
    "#         x = torch.flatten(x, 1)\n",
    "\n",
    "#         x = self.fc1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.fc2(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden_layers=50):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_layers = hidden_layers\n",
    "        # lstm1, lstm2, linear are all layers in the network\n",
    "        self.lstm1 = nn.LSTMCell(7, self.hidden_layers)\n",
    "        self.lstm2 = nn.LSTMCell(self.hidden_layers, self.hidden_layers)\n",
    "        self.linear = nn.Linear(self.hidden_layers, 1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) #first Dense\n",
    "        out = self.relu(out) #relu\n",
    "        out = self.fc(out) #Final Output\n",
    "\n",
    "    def forward(self, y, future_preds=0):\n",
    "        outputs, num_samples = [], y.size(0)\n",
    "        h_t = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        c_t = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        h_t2 = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        c_t2 = torch.zeros(n_samples, self.hidden_layers, dtype=torch.float32)\n",
    "        \n",
    "        for time_step in y.split(1, dim=1):\n",
    "            # N, 1\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t)) # initial hidden and cell states\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2)) # new hidden and cell states\n",
    "            output = self.linear(h_t2) # output from the last FC layer\n",
    "            outputs.append(output)\n",
    "            \n",
    "        for i in range(future_preds):\n",
    "            # this only generates future predictions if we pass in future_preds>0\n",
    "            # mirrors the code above, using last output/prediction as input\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs.append(output)\n",
    "        # transform list to tensor    \n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7538d61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000 #1000 epochs\n",
    "learning_rate = 0.001 #0.001 lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "print(model)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.LBFGS(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2aef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(data[['L1_Time_in_Zone',\n",
    "'L2_Time_in_Zone',\n",
    "'L3_Time_in_Zone',\n",
    "'L4_Time_in_Zone',\n",
    "'L5_Time_in_Zone',\n",
    "'L6_Time_in_Zone',\n",
    "'L7_Time_in_Zone']].to_numpy())\n",
    "y = torch.tensor(data['mod_pow_at_threshold'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180a4c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    outputs = model.forward(X) #forward pass\n",
    "    optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "    \n",
    "    # obtain the loss function\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    loss.backward() #calculates the loss of the loss function\n",
    "    \n",
    "    optimizer.step() #improve from loss, i.e backprop\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3a05aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysis_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "450c6c939a91a13d43daedb0706b943ba15a63d2bd7aa1f53ffd849b7045dd1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
